# -*- coding: utf-8 -*-
"""0304딥러닝.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e7T0_fx_Xa6oZ_ApHtH6WkGGUI8pxA1W
"""

from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((-1, 28, 28, 1))
test_images = test_images.reshape((-1, 28, 28, 1))

train_images = train_images/255.
test_images = test_images/255.

valid_images, test_images, valid_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.15, shuffle=True)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization

class ConvBNRelu(Model):
    def __init__(self, filters, kernel_size=3, strides=(1, 1), padding='same'):
        super(ConvBNRelu, self).__init__()
        self.conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, 
                           padding=padding, kernel_initializer='glorot_normal')
        
        self.batchnorm = BatchNormalization()

    def call(self, inputs, training=False):
        layer = self.conv(inputs)
        layer = self.batchnorm(layer)
        layer = tf.nn.relu(layer)

        return layer

class DenseBNRelu(Model):
    def __init__(self, units):
        super(DenseBNRelu, self).__init__()
        self.dense = Dense(units=units, kernel_initializer='glorot_normal')
        self.batchnorm = BatchNormalization()
    def call(self, inputs, training=False):
        layer = self.dense(inputs)
        layer = self.batchnorm(layer)
        layer = tf.nn.relu(layer)


        return layer

class MNISTModel(Model):
    def __init__(self):
        super(MNISTModel, self).__init__()
        self.conv1 = ConvBNRelu(filters=32, kernel_size=(3, 3), padding='valid')
        self.pool1 = MaxPool2D()
        self.conv2 = ConvBNRelu(filters=64, kernel_size=(3, 3), padding='valid')
        self.pool2 = MaxPool2D()
        self.conv3 = ConvBNRelu(filters=64, kernel_size=(3, 3), padding='valid')
        self.flat = Flatten()
        self.dense4 = DenseBNRelu(units=64)
        self.drop = Dropout(0.2)
        self.outputs = Dense(10, activation='softmax')

    def call(self, inputs, training=False):
        net = self.conv1(inputs)
        net = self.pool1(net)
        net = self.conv2(net)
        net = self.pool2(net)
        net = self.conv3(net)
        net = self.flat(net)
        net = self.dense4(net)
        net = self.drop(net)
        net = self.outputs(net)

        return net

del model

model = MNISTModel()
model(Input(shape=(28, 28, 1)))
model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

MODEL_SAVE_FOLDER='/content/drive/MyDrive/models/'
model_path = f"{MODEL_SAVE_FOLDER}mnist-{{epoch:d}}-{{val_loss:.5f}}-{{val_accuracy:.5f}}.hdf5"

cb_checkpoint = ModelCheckpoint(filepath=model_path, 
                                monitor='val_accuracy', 
                                verbose=1,
                                save_weights_only=True,
                                save_best_only=True)

cb_early_stopping = EarlyStopping(monitor='val_accuracy', patience=6)

import numpy as np
import tensorflow as tf
from tensorflow import keras

learning_rate = 0.001
batch_size = 200

lr_decay = tf.keras.optimizers.schedules.ExponentialDecay(learning_rate, train_images.shape[0]/batch_size*5, 0.5, staircase=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_decay)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

hist = model.fit(train_images, train_labels, validation_data=(valid_images, valid_labels),
                 epochs=100, batch_size=batch_size, 
                 callbacks=[cb_checkpoint, cb_early_stopping])

import matplotlib.pyplot as plt

fig, loss_ax = plt.subplots()
acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train_loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='valid_loss')
loss_ax.set_xlabel('epochs')
loss_ax.set_ylabel('loss')
loss_ax.legend(loc='upper left')

acc_ax.plot(hist.history['accuracy'], 'b', label='train_accuracy')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='valid_accuracy')
acc_ax.set_xlabel('epochs')
acc_ax.set_ylabel('accuracy')
acc_ax.legend(loc='upper right', bbox_to_anchor=(1, 0.5))

plt.show()

from tensorflow.keras.applications import VGG16
from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split
import numpy as np
import tensorflow as tf
from tensorflow import keras

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
print(train_images.shape, test_images.shape)

train_images = train_images.reshape(-1, 784).astype('float32')
test_images = test_images.reshape(-1, 784).astype('float32')

print(train_images.shape, test_images.shape)

# 3 채널로 만들기
train_images = np.dstack([train_images]*3)
test_images = np.dstack([test_images]*3)

#이미지 형식으로 변경
train_images = train_images.reshape(-1, 28, 28, 3)
test_images = test_images.reshape(-1, 28, 28, 3)

train_images.shape

from tensorflow.keras.preprocessing.image import img_to_array, array_to_img

#48*48 사이즈로 변경

train_images = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48, 48))) for im in train_images])
test_images = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48, 48))) for im in test_images])

train_images = train_images/ 255.
test_images = test_images / 255.

valid_images, test_images, valid_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.15, shuffle=True)

vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))
vgg_model.summary()

train_images = train_images/ 255.
test_images = test_images / 255.



vgg_model = VGG16(weights= 'imagenet', include_top=False, input_shape=(48, 48, 3))
vgg_model.summary()

layer_dict = {layer.name : layer for layer in vgg_model.layers}
layer_dict

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization

x= layer_dict['block2_conv2'].output

x=Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)
x=MaxPool2D()(x)
x=Flatten()(x)
x=Dense(256,activation='relu')(x)
x=Dropout(0.3)(x)
x=Dense(10,activation='softmax')(x)

custom_model=Model(inputs=vgg_model.input, outputs=x)

custom_model.summary()

for layer in custom_model.layers[:7]:
    layer.trainable = False

custom_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

MODEL_SAVE_FOLDER='/content/drive/MyDrive/models/'
model_path = f"{MODEL_SAVE_FOLDER}mnist-{{epoch:d}}-{{val_loss:.5f}}-{{val_accuracy:.5f}}.hdf5"

cb_checkpoint = ModelCheckpoint(filepath=model_path, 
                                monitor='val_accuracy', 
                                verbose=1,
                                save_weights_only=True,
                                save_best_only=True)

cb_early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)

hist = custom_model.fit(train_images, train_labels, validation_data=(valid_images, valid_labels),epochs=100, batch_size=200, callbacks=[cb_checkpoint,cb_early_stopping])

!ls -la '/content/drive/MyDrive/models/' #파일이름이랑 용량도 보여줌

saved_path = "/content/drive/MyDrive/models/mnist-9-1.76045-0.38953.hdf5"
custom_model.load_weights(saved_path)

custom_model.evaluate(test_images, test_labels)

