# -*- coding: utf-8 -*-
"""2016딥러닝.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F8QjBK0SfncYFpYcYfHRpfYgNI0Da_kf
"""

import numpy as np

y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
t= [0,0,1,0,0,0,0,0,0,0]



def sum_sqaures_error(y,t): # 평균제곱 오차
  return 0.5*np.sum((y-t)**2).round(3)

sum_sqaures_error(np.array(y),np.array(t))

y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
sum_sqaures_error(np.array(y),np.array(t))


def cross_entropy(Y,P): #2진분류
  Y = np.array(Y)
  P = np.array(P)
  
  return -np.sum(Y*np.log(P)+(1-Y)*np.log(1-P)).round(3)  # 맥시멈 라이클리후드

P = [[0.6, 0.2, 0.9, 0.3], [0.7, 0.9, 0.2, 0.4]]
Y = [[1., 1., 0., 0.], [1., 1., 0., 0.]]
for p,y in zip(P,Y):
  print(cross_entropy(y,p))



def cross_entropy_error(y,t):
  delta = 1e-7 #1의 -7승 y 가 0이면 로그가 무한대로 감 그래서 줌
  return -np.sum(t*np.log(y+delta)).round(3)

"""멀티풀 플레시피케이션"""

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]  #멀티풀 플레시피케이션
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
cross_entropy_error(np.array(y),np.array(t))

y = [0.1, 0.05, 0.4, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 잘못 분류됨
cross_entropy_error(np.array(y),np.array(t))

np.array(y).shape[0] #1차원이라 그냥 데이터의 값

def cross_entropy_error(y,t):
  if y.ndim ==1: # y의 차원이 1이라면
    t = t.reshape(1,t.size) # 1차원이라 배치를 나눌수 없어서 차원을 나눠줌
    y = y.reshape(1,y.size)

  batch_size = y.shape[0]

  delta = 1e-7
  return -np.sum(t*np.log(y+delta))/batch_size #엘리멘트끼리 더해지고 샘플의 갯수(배치사이즈) 만큼 나눠 평균을 내줌

y = [[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0],[0.2, 0.05, 0.5, 0.0, 0.05, 0.05, 0.05, 0.1, 0.0, 0.0]]  #멀티풀 플레시피케이션
t = [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]


def numerical_diff(f,x): # 함수,변수
  h = 1e-4
  return (f(x+h)-f(x)) /h

def numerical_diff(f,x): # 양쪽으로 h만큼 띄운후 점점 줄이는 식. 기울기를 구할때 좀 더 정확함
  h = 1e-4
  return (f(x+h)-f(x-h)) /(2*h)

def func_1(x):
  return 0.01*x**2+0.1*x

numerical_diff(func_1,5)

numerical_diff(func_1,5)

import matplotlib.pylab as plt

def tangent_line(f,x): 
  d= numerical_diff(f,x) #기울기 구하기
  y = f(x)-d*x # 절편 구하기
  return lambda t: d * t + y

x= np.arange(0.0,20.0,0.1)
y= func_1(x)

tf = tangent_line(func_1,5)
y2= tf(x)

plt.xlabel("x")
plt.ylabel("f(x)")
plt.plot(x,y)
plt.plot(x,y2) #탄젠트 곡선 (직선)
plt.show()

def func_2(x): # x 즉 변수가 2개인 함수
  return x[0]**2 + x[1]**2 # 원의 그래프



def numerical_gredient(f,x):
  h = 1e-4
  grad = np.zeros_like(x) # 기울기를 구하는것 변수(x)가 두개이니 grad도 차원을 두개로  만들어 주는것  

  for idx in range(x.size): # x0부터 편미분하고 다돌면 x1도 편미분
    tmp_val = x[idx]

    #f(x + h)
    x[idx] = float(tmp_val) + h
    fxh1 = f(x)
    
    #f(x - h)

    x[idx] = float(tmp_val) - h
    fxh2 = f(x)

    grad[idx] = (fxh1-fxh2) / (2*h) # W값들,Weight
    x[idx] = tmp_val # 원복

  return grad

numerical_gredient(func_2, np.array([3.0,4.0]))


def gredient_descent(f, init_x,lr=0.01,step_num=100):
  x = init_x
  x_history = []

  for i in range(step_num):
    x_history.append(x.copy())

    grad = numerical_gredient(f,x)
    # 비용함수를 w0,w1에 대한 편미분
    x -= lr * grad # x = x- lr *grad 여기의 x가 ppt에서는 W

  return x, np.array(x_history)

init_x = np.array([-3.0,4.0])

lr = 0.1
step_num = 100

x, x_history = gredient_descent(func_2, init_x, lr=lr ,step_num=step_num)

x

x_history

np.set_printoptions(formatter = {'float_kind':lambda x : "{0:0.5f}".format(x)}) # 보기편하게 바꾸는 거

x_history

def softmax(x): 
  exp_x = np.exp(x)
  sum_exp_x = sum(exp_x)
  y = exp_x / sum_exp_x

  return y

def _numerical_gredient_no_batch(f,x): #편미분해주는것
  h = 1e-4
  grad = np.zeros_like(x)  

  for idx in range(x.size): 
    tmp_val = x[idx]

    #f(x + h)
    x[idx] = float(tmp_val) + h
    fxh1 = f(x)
    
    #f(x - h)

    x[idx] = float(tmp_val) - h
    fxh2 = f(x)

    grad[idx] = (fxh1-fxh2) / (2*h) 
    x[idx] = tmp_val # 원복

  return grad

def numerical_gredient(f, X): #샘플(배치만큼) 가져옴
  if X.ndim == 1:
    return _numerical_gredient_no_batch(f,X)
  else:
    grad = np.zeros_like(X)

    for idx, x in enumerate(X):
      grad[idx] =_numerical_gredient_no_batch(f,x)

    return grad

class SimpleNet: # 단층 신경망 생성
  def __init__(self):
    self.W = np.random.randn(2,3)

  def predict(self,x): 
    return np.dot(x, self.W)

  def loss(self,x,t):
    z = self.predict(x)
    y=softmax(z)
    loss = cross_entropy_error(y,t)

    return loss

net = SimpleNet()
net.W

x=np.array([0.6,0.9])

p= net.predict(x) #실제 예측값
p

np.argmax(p)

t = np.array([1,0,0]) #t = 라벨임
net.loss(x,t)

def f(W):
  return net.loss(x,t)
dw = numerical_gredient(f,net.W) #한번 목적함수f에 대해 미분한 값
dw

dw = numerical_gredient(lambda w : net.loss(x, t), net.W) # def f(W)대신 람다식
dw #편미분을 한값 (기울기)

net.W = net.W -0.001 * dw # 새로운 w업데이트 0.001은 러닝레이트
net.W -= 0.001 *dw

p= net.predict(x)
p

np.argmax(p)

net.loss(x, t) # 업데이트 되면서 로스가 작아짐

for i in range(11):
  dw = numerical_gredient(lambda w : net.loss(x, t), net.W)
  net.W = net.W -0.001 * dw
  loss = net.loss(x, t)
  print(loss)

