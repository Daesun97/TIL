# -*- coding: utf-8 -*-
"""0225딥러닝(텐서플로).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FsdR-NeK9LUy_BLHGvru351Ugyghzz1v
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

tf.__version__

#!pip install tensorflow==2.3.0 너무최신은 안돌아갈수가 있어서 이걸로 함

#안변하는 수
a= tf.constant(2.0)
y=tf.constant(8.0)

#변수
x=tf.Variable(10.0) #내부적으로 밸류에 대해서는 넘파이를 사용함, 넘파이 자료와 잘맞음

print(a)
print(x)

loss = tf.math.abs(a*x-y)  # y=목적함수, x= 알고싶은 값(weight) a=상수
loss

loss.numpy()

def train_func():
    with tf.GradientTape() as tape:  #자동미분사용
        loss = tf.math.abs(a*x-y) #loss함수에서 loss구하기 #loss는 식에대한 정보를 가지고 있는 'tensor'임
        print("loss:{},type:{}".format(loss, type(loss)))
        dx = tape.gradient(loss, x) # loss함수에 대해 x로 미분(기울기값나옴)
        print("x={}, dx={}".format(x.numpy(),dx)) 

    x.assign(x - dx) #학습률을 1로 가정 (가중치를 업데이트)
        # assign 갱신
for i in range(4):
    train_func()

"""![image.png](data:image/png;base64,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)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# x = np.array(np.arange(-5,5,0.1))
# y = 2*x*x+3*x+5
# lr = 0.001
# 
# w1 = tf.Variable(1.0)
# w2 = tf.Variable(1.0)
# b = tf.Variable(1.0)
# 
# histloss = []
# for epoch in range(10000):
#     with tf.GradientTape() as tape:
#         # .sqrt =  루트씌우는 거 , .reduce_mean = 평균내는거
#         loss = tf.sqrt(tf.reduce_mean(tf.square(w1*x*x + w2*x + b - y))) #시그마뒤에식 
# 
#     dw1, dw2, db = tape.gradient(loss, [w1, w2, b])
# 
#     #w1.assign(w1-lr*dw1)
#     w1.assign_sub(lr*dw1) #위에거랑 같은식임
#     w2.assign_sub(lr*dw2)
#     b.assign_sub(lr*db)
# 
#     histloss.append(loss)
#     if epoch %50 ==0:
#         print(f"epoch={epoch}, loss={loss}")
# 
# 
#

plt.plot(histloss,color='red', linewidth=1)
plt.title("Loss func")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

from tensorflow.keras import optimizers

# Commented out IPython magic to ensure Python compatibility.
# %%time
# x = np.array(np.arange(-5,5,0.1))
# y = 2*x*x+3*x+5 # w1*x*x + w2*x + b
# lr = 0.001
# 
# w1 = tf.Variable(1.0) #초기값 설정
# w2 = tf.Variable(1.0)
# b = tf.Variable(1.0)
# var_list = [w1, w2, b]
# 
# # Momentum 적용
# opt=optimizers.SGD(learning_rate=lr, momentum=0.7)
# 
# histloss = []
# for epoch in range(10000):
#     with tf.GradientTape() as tape:
#         # .sqrt =  루트씌우는 거 , .reduce_mean = 평균내는거
#         loss = tf.sqrt(tf.reduce_mean(tf.square(w1*x*x + w2*x + b - y))) #시그마뒤에식 
# 
#     grads = tape.gradient(loss,var_list)
# 
#     opt.apply_gradients(zip(grads, var_list))#가중치 업데이트
# 
#     histloss.append(loss)
#     if epoch %500 ==0:
#         print(f"epoch={epoch}, loss={loss}")
#         print(f"w1: {w1.numpy()}, w2: {w2.numpy()}")
# 
# 
#

plt.plot(histloss,color='red', linewidth=1)
plt.title("Loss func(Momentum)")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# x = np.array(np.arange(-5,5,0.1))
# y = 2*x*x+3*x+5 # w1*x*x + w2*x + b
# lr = 0.001
# 
# w1 = tf.Variable(1.0) #초기값 설정
# w2 = tf.Variable(1.0)
# b = tf.Variable(1.0)
# var_list = [w1, w2, b]
# 
# # Momentum 적용
# opt=optimizers.Adam(learning_rate=lr)
# 
# def loss(): # 로스를 함수로 변환
#     return tf.sqrt(tf.reduce_mean(tf.square(w1*x*x + w2*x + b - y)))
# 
# 
# histloss = []
# for epoch in range(10000):
#     opt.minimize(loss, var_list=var_list) #밑에 두개를 한방에 처리함 , gradient 구하고 가중치 업데이트까지
#         # grads = tape.gradient(loss,var_list)
#         # opt.apply_gradients(zip(grads, var_list))#가중치 업데이트
# 
#     histloss.append(loss()) #함수로 넣고
#     if epoch %500 ==0:
#         print(f"epoch={epoch}, loss={histloss[-1]}") #함수의 맨 마지막 값 호출
#

plt.plot(histloss,color='red', linewidth=1)
plt.title("Loss func(Adam)")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

"""Keras"""

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

x = np.array(np.arange(-5,5,0.1))
y = 2*x*x+3*x+5 # w1*x*x + w2*x + b
lr = 0.001
dataX = np.stack([x*x, x]).T # 샘플의 갯수, 입력의 갯수

np.array([x*x,x]).shape

np.array([x*x,x]).T.shape

model = Sequential()
model.add(Dense(1, input_dim=2)) # y값 하나만 넣으니 1차원 모델 구조
model.compile(loss='mse', optimizer=optimizers.RMSprop(lr=0.05)) # 모델 완성
h= model.fit(dataX,y,batch_size=10, epochs=300) # for 문 안돌리고 fit함

plt.plot(h.history['loss'],color='red', linewidth=1)
plt.title("Loss func(Keras)")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

model.summary()

params= model.layers[0].get_weights() #w에 대한값
print("w1 : {:.2}".format(params[0][0][0]))
print("w2 : {:.2}".format(params[0][1][0]))
print("b : {:.2}".format(params[1][0]))

"""**Functional API**"""

from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model

x = np.array(np.arange(-5,5,0.1))
y = 2*x*x+3*x+5 # w1*x*x + w2*x + b
dataX = np.stack([x*x, x]).T # 샘플의 갯수, 입력의 갯수

xInput = Input(batch_shape=(None, dataX.shape[1]))
yOutput = Dense(1)(xInput)
model = Model(xInput,yOutput)
model.compile(loss="mse", optimizer=optimizers.Adam(learning_rate=0.05))

h = model.fit(dataX, y, batch_size=10, epochs=300)

plt.plot(h.history['loss'],color='red', linewidth=1)
plt.title("Loss func(Functional API)")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

params= model.layers[1].get_weights() #w에 대한값
print("w1 : {:.2}".format(params[0][0][0]))
print("w2 : {:.2}".format(params[0][1][0]))
print("b : {:.2}".format(params[1][0]))

model.summary()

import tensorflow.compat.v1 as tf  #버전호환

tf.disable_v2_behavior()

import numpy as np
import pandas as pd
from sklearn import datasets

iris= datasets.load_iris()
iris_X = iris.data
iris_y = pd.get_dummies(iris.target).to_numpy()  # 원핫인코딩 넘파이로 들고옴

iris_y[:10]

from sklearn.model_selection import train_test_split

train_X, test_X, train_y, test_y = train_test_split(iris_X,iris_y, test_size=0.3, random_state=42)

trint_X.shape #피쳐의 수 4

# 변수 선언 요세는 이렇게 안함
x = tf.placeholder(tf.float32,[None,4])
y = tf.placeholder(tf.float32,[None,3])

W = tf.Variable(tf.zeros([4,3]))
b = tf.Variable(tf.zeros([3]))

# 출력
h = tf.nn.softmax(tf.matmul(x,W)+b) #matmul=dot 행렬곱

#손실 함수 정의
cross_entropy = -tf.reduce_sum(y*tf.log(h), reduction_indices=[1])
loss = tf.reduce_mean(cross_entropy)

# 학습정의 - loss함수, optimizer
train = tf.train.GradientDescentOptimizer(0.001).minimize(loss)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

for i in range(10000):
    _, loss_values = sess.run([train,loss], feed_dict={x:train_X, y:train_y})
    if i % 1000 == 0:
        print(i, loss_values)

correct_prediction = tf.equal(tf.argmax(h, 1), tf.argmax(y, 1))

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
accuracy

print(sess.run(accuracy, feed_dict={x:test_X, y:test_y}))

import tensorflow as tf

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import optimizers

n_features = train_X.shape[1]
model = Sequential()
model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features, ))) #히든레이어
model.add(Dense(8,activation='relu', kernel_initializer='he_normal')) #히든레이어
model.add(Dense(3, activation='softmax')) #마지막 레이어

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(train_X, np.argmax(train_y, axis=1),epochs=300, batch_size=10 )

loss, acc = model.evaluate(test_X, np.argmax(test_y, axis=1), verbose=0)
print(f"test accuracy: {acc}")

model.compile(optimizer=optimizers.Adam(learning_rate=0.001, decay=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(train_X, np.argmax(train_y, axis=1),epochs=300, batch_size=10, verbose=0)
loss, acc = model.evaluate(test_X, np.argmax(test_y, axis=1), verbose=0)
print(f"test accuracy: {acc}")

"""**Functional API**"""

xInput = Input(batch_shape=(None,4))
yOutput = Dense(3)(xInput)
model = Model(xInput, yOutput)
model.compile(loss='mse',optimizer=optimizers.Adam(lr=0.01))
model.fit(train_X,train_y, epochs=200)

xInput = Input(batch_shape=(None,4))
yOutput = Dense(3)(xInput)
model = Model(xInput, yOutput)
model.compile(optimizer=optimizers.Adam(learning_rate=0.001, decay=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(train_X, np.argmax(train_y, axis=1),epochs=300, batch_size=32, verbose=0)
loss, acc = model.evaluate(test_X, np.argmax(test_y, axis=1), verbose=0)
print(f"test accuracy: {acc}")

n_features = train_X.shape[1]

input_layer = Input(shape=(n_features, )) #1차원이라 뒤에 비워둘 수 있음
hidden_layer_1 = Dense(10)(input_layer)
hidden_layer_2 = Dense(8)(hidden_layer_1)
output_layer = Dense(3, activation='softmax')(hidden_layer_2)
model = Model(input_layer, output_layer)

model.compile(optimizer=optimizers.Adam(learning_rate=0.002, decay=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(train_X, np.argmax(train_y, axis=1),epochs=300, batch_size=10, verbose=0)
loss, acc = model.evaluate(test_X, np.argmax(test_y, axis=1), verbose=0)
print(f"test accuracy: {acc}")

