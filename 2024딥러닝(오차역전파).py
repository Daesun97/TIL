# -*- coding: utf-8 -*-
"""2024딥러닝(오차역전파).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B-7LDzFrcB1IfyqYU9eO6QkmLgDQU-gJ
"""

import numpy as np

class MulLayer:
    def __init__(self): #곱셈노드
        self.x=None
        self.y=None

    def forward(self,x,y):
        self.x=x
        self.y=y
        return x*y

    def backward(self, dout): #dout= 상류에서 흘러오는 값
        dx = dout * self.y
        dy = dout * self.x

        return dx, dy

apple = 100
apple_num = 2
tax = 1.1

mul_apple_layer = MulLayer()
mul_tax_layer = MulLayer()

#forward
apple_price = mul_apple_layer.forward(apple, apple_num)
price = mul_tax_layer.forward(apple_price, tax)

#backward
d_price = 1
d_apple_price , d_tax = mul_tax_layer.backward(d_price)
d_apple, d_apple_num = mul_apple_layer.backward(d_apple_price)

print(f"price: {price}")
print(f"d_apple : {d_apple}")
print(f"d_apple_num: {d_apple_num}")
print(f"d_tax : {d_tax}")

class AddLayer: #포워드와 백워드 구하기
    # def __init__(self): #덧셈노드
    #     self.x=None
    #     self.y=None
    
    def forward(self,x,y):
        self.x=x
        self.y=y 
        return x + y

    def backward(self, dout): #dout= 상류에서 흘러오는 값
        dx = dout 
        dy = dout  
         

        return dx, dy

apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
mul_tax_layer = MulLayer()

add_mix_layer = AddLayer()

#forward
apple_price = mul_apple_layer.forward(apple, apple_num)
orange_price = mul_orange_layer.forward(orange, orange_num)
mix_price = add_mix_layer.forward(apple_price, orange_price)
price = mul_tax_layer.forward(mix_price, tax)


#backward
d_price = 1
d_mix_price , d_tax = mul_tax_layer.backward(d_price)
d_apple_price, d_orange_price = add_mix_layer.backward(d_mix_price)
d_apple, d_apple_num = mul_apple_layer.backward(d_apple_price)
d_orange, d_orange_num = mul_orange_layer.backward(d_orange_price)



print(f"price: {price}")
print(f"d_apple : {d_apple}")
print(f"d_apple_num: {d_apple_num}")
print(f"d_orange : {d_orange}")
print(f"d_orange_num: {d_orange_num}")
print(f"d_tax : {d_tax}")

"""Relu 역전파 (요세 더 많이 씀)"""

class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x): # 대부분 넘파이 어레이로 들어옴
        self.mask = (x<= 0)
        out = x.copy() # x를 변형시키지 않으려고 씀
        out[self.mask] = 0 #0보다 작거나 같은 인자의 인덱스를 영으로 전환
        return out

    def backward(self, dout):
        dout[self.mask] = 0
        #dx= dout
        return dout

"""Sigmoid 역전파"""

def sigmoid(x):
  return 1 / (1+np.exp(-x))

class Sigmoid:
    def __init__(self):
        self.out = None
    def forward(self, x):
        self.out = sigmoid(x)
        return self.out

    def backward(self, dout):
        dx = dout * self.out*(1-self.out)
        return dx

"""행렬곱은 대응하는 차원의 원소수가 일치되어야 함
그런 행렬을 Affine행렬이라고 함
다차원행렬의 미분을 해야하는데 설명 개어려우니 걍 차원을 맞추는 것만 기억하기 (트랜스포즈로)

"""

class Affine:
    def __init__(self, W, b):  
        self.W = W
        self.b = b

        self.x = None
        self.dw = None
        self.db = None
    
    def forward(self, x):
        self.x = x

        out = np.dot(self.x, self.W) + self.b
        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)

        self.dw = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)

        return dx

def softmax(x): 
    exp_x = np.exp(x)
    sum_exp_x = np.sum(exp_x, axis= -1).reshape(-1,1) # axis= -1은 맨 마지막 차원에 관한것
    y = exp_x / sum_exp_x

    return y

def cross_entropy_error(y,t):
    if y.ndim ==1: # y의 차원이 1이라면
        t = t.reshape(1,t.size) # 1차원이라 배치를 나눌수 없어서 차원을 나눠줌
        y = y.reshape(1,y.size)

    batch_size = y.shape[0]

   
    delta = 1e-7
    return -np.sum(t * np.log(y+delta)) / batch_size

class SoftmaxwithLoss():
    def __init__(self):
        self.loss = None
        self.y = None #  
        self.t = None # 라벨값


    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t) 
        
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
       
        if self.t.size == self.y.size: # y는 원핫인코딩으로 나옴, 근데 t는 라벨인코딩으로 나옴 원핫인코딩으로 맞추기
            dx = (self.y - self.t)/batch_size
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size),self.t] -= 1
            dx = dx/batch_size

        return dx

def _numerical_gredient_no_batch(f,x): #편미분해주는것
  h = 1e-4
  grad = np.zeros_like(x)  

  for idx in range(x.size): 
    tmp_val = x[idx]

    #f(x + h)
    x[idx] = float(tmp_val) + h
    fxh1 = f(x)
    
    #f(x - h)

    x[idx] = float(tmp_val) - h
    fxh2 = f(x)

    grad[idx] = (fxh1-fxh2) / (2*h) 
    x[idx] = tmp_val # 원복

  return grad

def numerical_gredient(f, X): #샘플(배치만큼) 가져옴
  if X.ndim == 1:
    return _numerical_gredient_no_batch(f,X)
  else:
    grad = np.zeros_like(X)

    for idx, x in enumerate(X):
      grad[idx] =_numerical_gredient_no_batch(f,x)

    return grad

from collections import OrderedDict
class TwoLayerNet:
    def __init__(self, input_size,hidden_size,output_size,weight_init_std=0.01): # 차라미터 랜덤 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

        self.layers = OrderedDict()  #이렇게 흘러감
        self.layers["Affine1"] = Affine(self.params['W1'], self.params['b1'])
        self.layers["Relu"] = Relu()
        self.layers["Affine2"] = Affine(self.params['W2'], self.params['b2'])
        self.lastlayer = SoftmaxwithLoss()

    def predict(self, x): #포워드 돌리기
        for layer in self.layers.values(): #딕셔너리임
            x = layer.forward(x)

        return x #로짓(소프트맥스 전 값)


    def loss(self, x, t):
        y = self.predict(x)
    
        return self.lastlayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: #라벨이 아니면
            t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t)/float(x.shape[0])

        return accuracy

    def numerical_gredient(self, x, t): # 각파라미터 기울기 구하기
        loss_W = lambda W : self.loss(x,t) # 목적함수 - Cross Entropy

        grads = {}
        #목적함수에 대해 각 파라미터별로 편미분한것 
        grads['W1'] = numerical_gredient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gredient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gredient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gredient(loss_W, self.params['b2'])

        return grads


    def gradient(self, x, t):
        self.loss(x, t)

        dout = 1
        dout = self.lastlayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()

        for layer in layers:
            dout = layer.backward(dout)

        grads = {}
        grads['W1'] = self.layers['Affine1'].dw
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dw
        grads['b2'] = self.layers['Affine2'].db
        
        return grads

import pickle
def load_mnist(normalize=True, flatten=True, one_hot_label=False):
  def _change_one_hot_label(x): # 라벨을 결정하는것 숫자가 7이라면 7번째 0이 1로 바뀜
    T = np.zeros((x.size, 10))
    for idx ,row in enumerate(T):
      row[x[idx]] = 1
    return T
  

  with open('/content/drive/MyDrive/실습/mnist.pkl의 사본','rb') as f:
    dataset = pickle.load(f)

  if normalize:
    for key in ('train_img', 'test_img'):
      dataset[key] = dataset[key].astype(np.float32)
      dataset[key] /= 255.0

  if one_hot_label:
    dataset['train_label'] = _change_one_hot_label(dataset['train_label'])
    dataset['test_label'] = _change_one_hot_label(dataset['test_label'])



  if not flatten:
    for key in ('train_img','test_img'):
      dataset[key] = dataset[key].reshape(-1,1,28,28) #샘플의 갯수 ,색감체널수, 픽셀의 수28x28
  
  return (dataset['train_img'],dataset['train_label']),(dataset['test_img'],dataset['test_label'])

(x_train,y_train),(x_test,y_test)= load_mnist(normalize=True,flatten=True, one_hot_label=True) #데이터 로딩

network = TwoLayerNet(input_size=784, hidden_size=50,output_size=10) # 2층 신경망 객체 생성

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.01 #학습률

train_loss_list = []
train_acc_list = []
test_acc_list=[]

iter_per_epoch = max(train_size/batch_size, 1)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size,batch_size) # train_size에서 랜덤하게 숫자를 batch_size만큼 들고옴 , 인덱스
    x_batch = x_train[batch_mask] #랜덤으로 훈련데이터에서 batch_size만큼 선택
    y_batch = y_train[batch_mask]# 얘는 라벨이서

    grad = network.gradient(x_batch,y_batch) #각 파라미너터의 gradient 계산

    for key in ('W1','b1','W2','b2'):  # 각 파라미터 업데이트  EX, w = w - 학습률 * 기울기(미분값)
        network.params[key] -=learning_rate * grad[key]


    loss = network.loss(x_batch, y_batch)
    train_loss_list.append(loss) # loss history = 로스값의 경과
    
    if i % iter_per_epoch ==0:
        train_acc = network.accuracy(x_train, y_train) # 업데이틔 후의 훈련데이터 정확도
        test_acc = network.accuracy(x_test, y_test)# 업데이틔 후의 테스트데이터 정확도
        
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

        print(f"loss {loss}, train_accuracy {train_acc}, test_accuracy {test_acc}")

import matplotlib.pylab as plt

x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label="train acc")
plt.plot(x, test_acc_list, label="test acc")
plt.xlabel("epoch")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc="lower right")
plt.show()

