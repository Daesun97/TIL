# -*- coding: utf-8 -*-
"""0215딥러닝.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O3vRADHdSlzclK9DpGKB-hx8uFyUMnN0
"""

!nvidia-smi

!gegorce-smi

!geforce-smi

import numpy as np

def AND(x1,x2):  #간단한 퍼셉트론
  w1,w2,theta = 1.0, 1.0, 1.2

  temp = x1*w1 + x2*w2
  if temp <= theta:
    return 0
  else:
    return 1



data = [(0,0),(1,0),(0,1),(1,1)]
for datum in data:
  y= AND(datum[0], datum[1])
  print(f"{datum}-> {y}" )

def AND(x1,x2):
  x = np.array([x1,x2])
  w = np.array([0.5, 0.5])
  b = -0.7
  # temp = np.sum(x*w) + b 내적과 같음
  temp = np.dot(x,w) + b 
  if temp <= 0:
    return 0
  else:
    return 1

data = [(0,0),(1,0),(0,1),(1,1)]
for datum in data:
  y= AND(datum[0], datum[1])
  print(f"{datum}-> {y}" )

def NAND(x1, x2):
  x = np.array([x1,x2])
  w = np.array([0.5, 0.5])
  b = -0.7
  # temp = np.sum(x*w) + b 내적과 같음
  temp = np.dot(x,w) + b 
  if temp > 0:
    return 0
  else:
    return 1

data = [(0,0),(1,0),(0,1),(1,1)]
for datum in data:
  y= NAND(datum[0], datum[1])
  print(f"{datum}-> {y}" )

def OR(x1,x2):
  x = np.array([x1,x2])
  w = np.array([0.5, 0.5])
  b = -0.4
  # temp = np.sum(x*w) + b 내적과 같음
  temp = np.dot(x,w) + b 
  if temp <= 0:
    return 0
  else:
    return 1

data = [(0,0),(1,0),(0,1),(1,1)]
for datum in data:
  y= OR(datum[0], datum[1])
  print(f"{datum}-> {y}" )

def XOR(x1, x2): #다층 퍼셉트론
  s1= NAND(x1,x2)
  s2 = OR(x1,x2)
  y= AND(s1,s2)
  return y

data = [(0,0),(1,0),(0,1),(1,1)]
for datum in data:
  y= XOR(datum[0], datum[1])
  print(f"{datum}-> {y}")

def sigmoid(x):
  return 1 / (1+np.exp(-x)) #exp = 2.718어쩌고

x= np.array([-1.0,1.0,2.0])
sigmoid(x) #0과 1로 분류됨 이진분류할때 많이 씀

def init_network():
  network = {} #모든가중치
  network['w1']=np.array([[0.1, 0.3, 0.5],[0.2, 0.4, 0.6]]) #2x3 
  network['b1'] = np.array([0.1,0.2,0.3])
  network['w2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]]) # 3x 2
  network['b2'] = np.array([0.1,0.2])
  network['w3'] = np.array([[0.1,0.3],[0.2,0.4]]) #2x2
  network['b3'] = np.array([0.1,0.4])

  return network


def forword(network, x): #전방향 계산식
  w1, w2, w3 = network['w1'],network['w2'],network['w3']
  b1, b2, b3 = network['b1'],network['b2'],network['b3']

  a1 = np.dot(x,w1)+b1
  z1 = sigmoid(a1) #h()
  a2 = np.dot(z1, w2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2,w3) + b3
  y= a3

  return y

network = init_network()
x= np.array([1.0,0.5])
y= forword(network,x)
print(y)

def softmax(x): #멀티레이어의 아웃풋으로 볼수있도록 
  exp_x = np.exp(x)
  sum_exp_x = sum(exp_x)
  y = exp_x / sum_exp_x

  return y

print(softmax([1,2,0]))

import pickle
def load_mnist(normalize=True, flatten=True, one_hot_label=False):
  def _change_one_hot_label(x): # 라벨을 결정하는것 숫자가 7이라면 7번째 0이 1로 바뀜
    T = np.zeros((X.size, 10))
    for idx ,row in enumerate(T):
      row[x[idx]] = 1

  return T
  

  with open('/content/drive/MyDrive/실습/mnist.pkl의 사본','rb') as f:
    dataset = pickle.load(f)

  if normalize:
    for key in ('train_img', 'test_img'):
      dataset[key] = dataset[key].astype(np.float32)
      dataset[key] /= 255.0

  if one_hot_label:
    dataset['train_label'] = _change_one_hot_label(dataset['train_label'])


  if not flatten:
    for key in ('train_img','test_img'):
      dataset[key] = dataset[key].reshape(-1,1,28,28) #샘플의 갯수 ,색감체널수, 픽셀의 수28x28
  
  return (dataset['train_img'],dataset['train_label']),(dataset['test_img'],dataset['test_label'])

(x_train,y_train),(x_test,y_test)= load_mnist(normalize=True,flatten=True, one_hot_label=False)

with open('/content/drive/MyDrive/실습/mnist.pkl의 사본','rb') as f:
   dataset = pickle.load(f)
   print(dataset.keys())

from google.colab import drive

import numpy as np
for key in ('train_img', 'test_img'):
   dataset[key] = dataset[key].astype(np.float32)
   dataset[key] /= 255.0

dataset['train_img'][0]

def _change_one_hot_label(x):
  T = np.zeros((X.size, 10))
  for idx ,row in enumerate(T):
    row[x[idx]] = 1

  return T



dataset['train_img'].reshape(-1,1,28,28)[0]

dataset['train_label'][1]

# (x_train,y_train),(x_test,y_test)= load_mnist(normalize=False,flatten=True, one_hot_label=False)
# from numpy.core.records import fromarrays
# from PIL import Image

# img = x_train[0]
# label = y_train[0]
# print(label
      
# print(img.shape)
# img = img.reshape(28,28)
# print(img.shape)

# pli_img = Image.fromarray(np.unit8(img))
# pil_img.show()

import matplotlib.pyplot as plt

def displayData(X,Y):
  fig, ax = plt.subplots(nrows=10, ncols=10, figsize=(15,15))
  fig.suptitle('Display randomly images of the training data set')

  for i in range(10):
    for j in range(10):
      ind = np.random.randint(X.shape[0])
      tmp=X[ind,:].reshape(28,28)
      ax[i,j].set_title("label : {}".format(Y[ind]))
      ax[i,j].imshow(tmp,cmap='gray_r')
      plt.setp(ax[i,j].get_xticklabels(), visible=True)
      plt.setp(ax[i,j].get_yticklabels(), visible=True)

  fig.subplots_adjust(hspace=0.5,wspace=0.5)

displayData(x_train,y_train)

def init_network():
  with open('/content/drive/MyDrive/실습/sample_weight.pkl', 'rb') as f:
    network = pickle.load(f)
  return network


def predict(network, x):
  W1,W2,W3 = network['W1'],network['W2'],network['W3']
  b1,b2,b3 = network['b1'],network['b2'],network['b3']

  a1 = np.dot(x,W1) + b1
  z1 = sigmoid(a1)

  a2 = np.dot(z1,W2) + b2
  z2= sigmoid(a2)

  a3 = np.dot(z2,W3) + b3
  y= softmax(a3)

  return y

model = init_network()

model.keys()

model['W1'].shape  # 입력이 784개, 히든레이어의 노드갯수가 50개

model['W2'].shape

model['W3'].shape

model['b1'].shape

x_test.shape

# Commented out IPython magic to ensure Python compatibility.
# %%time
# accuracy_cnt = 0
# for i in range(len(x_test)):
#   y= predict(model,x_test[i])
#   p = np.argmax(y) #가장큰애를 들고옴
#   if p ==y_test[i]:
#     accuracy_cnt +=1
# 
# print("Accuracy : {}".format(accuracy_cnt/len(x_test))) # 하나씩 연산하는것 100개면 100번

# Commented out IPython magic to ensure Python compatibility.
# %%time
# network = init_network()
# accuracy_cnt = 0
# batch_size = 100
# 
# for i in range(0,len(x_test),batch_size):
#   x_batch = x_test[i:i+batch_size] # x트레인은 6만개인데 100개만 들고옴
#   y_batch = predict(network,x_batch)#y_train[i:i+batch_size] 
# 
#   p = np.argmax(y_batch,axis=1) #100x10행률중 10개중에서 높은것을 찾기 위함
#   accuracy_cnt += np.sum(p == y_test[i:i+batch_size]) # 100개를 한번에 던져서 찾으라 하는것
# print("Accuracy : {}".format(accuracy_cnt/len(x_test))) # 배치사이즈만큼 연산하는것 훨씬 빠름

